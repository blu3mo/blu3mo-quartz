---
title: クロスエントロピー
---

from *東大1S情報α*

* *$ *
* 正解ラベルの[情報量](%E6%83%85%E5%A0%B1%E9%87%8F.md)として捉えられるらしい（？）
* *負の尤度関数*としても捉えられるらしい（？）
* よくわからん、そのうち分かりそう<img src='https://scrapbox.io/api/pages/blu3mo-public/blu3mo/icon' alt='blu3mo.icon' height="19.5"/>

---

* [クラス分類](%E3%82%AF%E3%83%A9%E3%82%B9%E5%88%86%E9%A1%9E.md)の時の、[Loss Function](Loss%20Function.md)になるやつ

* *自然対数*を確率にかけて、和を求める
  
  * ln(a)+ln(b) = ln(a\*b)を使う、logを使えば掛け算を避けられる
* *確率*（下のxの値、0~1）を全部かけた値が小さいほど、クロスエントロピーは大きくなる
  ![image](https://gyazo.com/613efa06a8741624a47c6b5b62f84efb/thumb/1000)

* 確率が0に近い = 損失は多い

* 確率が1に近い = 損失は少い

* あと、微分するとなんか良い感じになるのも理由の一つ?（たぶん）

* [PyTorch](PyTorch.md)では、nn.CrossEntropyLoss()
  
  * これは、LogSoftMaxと、NLLLossを合わせたもの
  * LogSoftmaxとNLLLossで同じ結果になる
