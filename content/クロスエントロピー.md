---
title:
 'クロスエントロピー'
---

from [[東大1S情報α]]
- [[$ ]]
- 正解ラベルの[[情報量]]として捉えられるらしい（？）
- [[負の尤度関数]]としても捉えられるらしい（？）
- よくわからん、そのうち分かりそう<img src='https://scrapbox.io/api/pages/blu3mo-public/blu3mo/icon' alt='blu3mo.icon' height="19.5"/>

---
- [[クラス分類]]の時の、[[Loss Function]]になるやつ
- [[自然対数]]を確率にかけて、和を求める
    - ln(a)+ln(b) = ln(a*b)を使う、logを使えば掛け算を避けられる
- [[確率]]（下のxの値、0~1）を全部かけた値が小さいほど、クロスエントロピーは大きくなる
![image](https://gyazo.com/613efa06a8741624a47c6b5b62f84efb/thumb/1000)
- 確率が0に近い = 損失は多い
- 確率が1に近い = 損失は少い

- あと、微分するとなんか良い感じになるのも理由の一つ?（たぶん）

- [[PyTorch]]では、nn.CrossEntropyLoss()
    - これは、LogSoftMaxと、NLLLossを合わせたもの
    - LogSoftmaxとNLLLossで同じ結果になる
