---
title:
 'ニューラルネットワークプロセッサ'
---

    - [[Domain Specific Architecture]]
    - [[論理回路]]フレンドリーな[[ディープラーニング]]を考える
        - 論理回路フレンドリーとは
            - 掛け算ではなく足し算、など
            - 論理演算だともっと嬉しい?
    - [[BRein]]
        - ポイント1: [[パーセプトロン]]の重み付け$w$を+1, -1のみに[[量子化]] ([[二値化]] binarise)する
            - さらに、+1, -1ではなく1,0にする
            - するとXNORの論理演算で重み付けの計算ができる
                - そうすると小さいメモリ/計算回路でNNができるように
        - ポイント2: メモリで計算をする
            - [[ニューラルネットワーク]]の構造をそのままメモリの回路構造に落とし込んだような
            - 一ビットのニューロンの情報が入ってきた時に、それを並列に次の層に(別々の重みで)流して、また並列に一つのところに集まるっていう計算
                - これをパイプライン化してまとめて行える?
    - [[QUEST]]
        - 量子化の部分がBReinと異なる、[[二値化]]ではなく対数量子化する
            - $w$の値は、0に近いことが多い
            - よって、0に近い部分をより細かく量子化している
        - 対数にして計算をすると、掛け算を足し算に変えられる
            - 「対数領域」で足し算 = 掛け算
            - 掛け算より足し算の方が小さい回路で済む、論理回路フレンドリー
        - 量子化のレベル（log3, log4, log5...）と認識精度の関係を調べると、
            - MNISTだとガクッと認識精度が下がる部分がある

    - 量子化には当然誤差が生まれる
        - その量子化誤差も[[評価関数]]に組みこむことで、認識誤差と量子化誤差の両方が小さくなるように学習させられる
        - 量子化誤差が少ないポイントに重みが自然(?)に集まる

    - 重みの[[ディザリング]]
        - 画像とかの[[ディザリング]]の考え方をニューラルネットの[[量子化]]に持ち込む
        - すると、認識精度をあげることができる
        - これをするために回路のサイズはほとんど変わらないので、低コストで認識精度があげられる

    - 動的無効ニューロン予測 (DNP)
        - [[ReLU]]とかは負値を0にする、0はもう0のまま (= 死んだ[[ニューロン]])
        - この無効ニューロンを予想できれば、その部分の計算をすっ飛ばせる
            - $x\times0=0$の計算はしたくない
        - どうするか
            - より軽い予測機構で先に死ぬニューロンを予測
            - その後、それらを省いたニューラルネットワークを作って効率的に学習
        - 結果
            - [[VGG]]でテスト
            - ある場合だと、5%程度精度下がるけど、計算リソースは半分で済む
            - 速さと精度の[[トレードオフ]]・チューニング
<img src='https://scrapbox.io/api/pages/blu3mo-public/情報科学の達人/icon' alt='情報科学の達人.icon' height="19.5"/>

#コンピューターアーキテクチャ
