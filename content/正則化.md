---
title:
 '正則化'
---

- [[機械学習]]で、回帰モデルの時にそれぞれの特徴量が与える影響を小さくすることで過学習を防ぐ
    - [[リッジ回帰]]は、調整が聞かない線形モデルに対して調整する、[[L2正則化]]
            - Alpha値を操作する
                - Alpha値が高いほど、係数への制約が大きくなる=凡化できる
                - 0に近づくと、効果が薄れる
                - Learning Curve
                    - ![image](https://gyazo.com/c7726152cc9523d8007343366658787c/thumb/1000)
                    - データーがめっちゃあるなら、[[正則化]]はあまり関係ないことがわかる
    - [[Lasso回帰]]([[L1正則化]])は、いくつかの係数が完全に0になることがある = より過激な正則化
- RidgeとLassoの使い分け
    - Ridgeをまず試す
    - もし、ほとんどの特徴量が意味ないことが想定されるなら、Lasso試す
    - シンプルなモデルが欲しい時も、Lasso試す
- 良いとこどりのElasticNetもある
    - 実用上はこれだけど、パラメーター二つ調整しないとで大変

- 分類問題では、下のグラフのように変化
    - - 正則化をどのくらいするかは、Cが決める
        - - Cが大きいと、正則化が弱くなる (アルファと逆)
        - - つまり、正則化が弱い => 過学習 => 細かい点に引っ張られやすくなる(3つ目のグラフ)
![image](https://gyazo.com/3db54f4d4059e3b36728f3b05c0f7a1a/thumb/1000)
#Pythonで始める機械学習
