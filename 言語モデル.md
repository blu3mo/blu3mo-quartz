---
title: 言語モデル
---

<img src='https://scrapbox.io/api/pages/blu3mo-public/情報科学の達人/icon' alt='情報科学の達人.icon' height="19.5"/> [自然言語処理](%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86.md)の講義

* 言語モデルの定義の一つ: 文の「*もっともらしさ*」を評価する
  
  * *音声認識*とかのいくつかの認識結果候補から一つを選ぶのにも使える
* *確率的言語モデル*
  
  * 「*文*」の数学的表現
    * 文s = <s> hello world </s>

      * <s></s>は、文頭/文末を表す単語的存在
  * ↑を用いて、文らしさを評価
    * P(a|b)は、bが来たあとのaの単語出現確率
    * P(<s>) * P(hello | <s>) * P(world | <s> hello) * P(</s> | <s> hello world)
    * 各単語について、それ以前の文章から考えてその単語がどのくらいありうるかを評価
    * P(a|b)をどうするか
      * *最尤推定*
        * *コーパス*の出現頻度で簡単に計算できる
        * 低頻度な現象には弱い
        * 0を返されるとP(a|b)の総乗が0になってしまう
      * *n-gram*による近似
        * 単語以前の全てではなく、単語以前のn単語のみを用いて*最尤推定*
        * nが小さいほど、低頻度な物に強くなる
        * nが多いほど長い文脈を考慮できる
          * *トレードオフ*
          * 機械翻訳n=4 (4-gram)までが一般的
      * [ニューラルネットワーク](%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.md)による推定
        * [RNN](RNN.md)に突っ込む
* 言語モデルは、*尤もらしさ*等を測る上で単語間の繋がりの情報を保持している
  
  * つまり、言語モデルは文章等を*ベクトル*にエンコード/デコードする物とも定義できる?
* [ニューラルネットワーク](%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.md)による*言語モデル*
  
  * [RNN](RNN.md)に突っ込んで*embedding* (*エンコード*)
    * 出力は[Softmax](Softmax.md)で0~1に正規化
  * [Attention機構](Attention%E6%A9%9F%E6%A7%8B.md)
    * 長い文になると、各単語が出力ベクトルに与える影響が小さくなってしまう
      * 出力ベクトルのサイズは固定
    * アテンションの重みを計算して、重要な単語を強く反映させる
  * [Transformer](Transformer.md)
    * [RNN](RNN.md)の*再帰*を無くして、注意機構のみでエンコード/デコード
  * ベクトルへのエンコードと、その別言語によるデコードができれば*機械翻訳*が出来る
* [GPT-3](GPT-3.md), [BERT](BERT.md)などは[Transformer](Transformer.md)の応用

* *Pre-trained*言語モデル
  
  * 様々なタスクに適応できる*言語モデル*
  * 大規模モデルはメンテナンス等にコストかかり、扱いにくい
    * 専用の小さいモデルと比べて、この点がデメリット
  * 軽量なモデル(*DistillBERT*とか)も作られている
* *人間の言語能力が強い言語モデルよりすごい理由*
