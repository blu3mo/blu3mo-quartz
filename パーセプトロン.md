---
title: パーセプトロン
---

[ニューラルネットワーク](%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.md)とかの元

* 一個だけの[機械学習](%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92.md)だと、表現力低いから使い物にならない

* *線形分離可能*じゃないといけない
  
  * クラス分け問題
* 方法
  
  * 重み[\[ベクトル\]](分ける線に対して90度)とxベクトルの内積の正負が、2つのベクトルが同じ側をむいているかどうかを表す
  * 同じ側を向いていない=*内積*が負だったら、重みベクトルをxベクトル+重みベクトルに更新
  * これを全データーでやる

## \#Pythonで始める機械学習

![image](https://gyazo.com/b6d97b5044e365fbd199e5ad640c28df/thumb/1000)

* パーセプトロンの表し方は二種類ある
  
  * 一つは、上の画像みたいに、Biasをインプットの一つ（値は常に1）にして、その重みがbiasになるというもの
  * もう一つは、バイアスをパーセプトロンの内部値として持つタイプ
  * 前者の方が多く使われるらしい
* AND operatorとかも、パーセプトロンで表現できる↓
  ![image](https://gyazo.com/660547594d166dc10842e35259f6aa74/thumb/1000)

* ↑のみたいなパーセプトロンを組み合わせるとXOR↓が作れる、シンプルなニューラルネットワーク?
  ![image](https://gyazo.com/13466e726789c384109cf416f69805fb/thumb/1000)
  \#Udacity_Intro_to_Deep_Learning_with_PyTorch

![image](https://gyazo.com/74288a41436b7541a6f8eb6738ea0021/thumb/1000)

* *シグモイド関数*とかを使って、出力を*連続*値にしないといけない
  
  * なぜなら、*離散*値だと少し動かした時に変化が起きないから
  * [Loss Function](Loss%20Function.md)と同じ
* シグモイド関数だと、xの値がある程度大きいとgradient（errorに使う値）がほとんど0になってしまう
  
  * その対策として、*ReLU*とか、*Tanh*とかの他の関数を使うこともある
    * ReLUは、正の値なら直値をそのまま返す、つまりgradientは1になる
    * Tanhはもうちょい複雑、でもシグモイド関数より良い感じのgradientを返す
      \#ディープラーニング
