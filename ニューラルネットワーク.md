---
title: ニューラルネットワーク
---

![image](https://gyazo.com/f9d48f693bcff67dc7a56eae02ffc70a/thumb/1000)
- 基本の回帰モデルを表すと↑になる

* これの層を増やしたり、間のユニットを増やすことで学習
  ![image](https://gyazo.com/db16bf5a95f6c84071dd4af48d95bc60/thumb/1000)

* 各矢印が、重みwを持っている、全部違う値
  
  * 学習によって調整される
* それだけだとただの回帰と同じ、だから、*ReLU*や*Tanh*を使ってフィルターをかける

* *リッジ回帰*みたいに、[正則化](%E6%AD%A3%E5%89%87%E5%8C%96.md)して重みを0に近づけることもできる
  
  * デフォルトはほとんど正則化しない
* 最初は、乱数で重みを決める

* 学習した内容の解析が難しい、やる方法の一つは重みのヒートマップをみること

* パラメーター学習の*アルゴリズム*には、*adam*や*lbfgs*等が初心者向けにある
  \#Pythonで始める機械学習

---

![image](https://gyazo.com/d922f2375a6ef10632b128cf874d61b7/thumb/1000)
モデルが完成したら、実際に[予測](%E4%BA%88%E6%B8%AC.md)する時はこの計算をすればいいだけ、簡単
（xが入力、Wが各層の重み、yが出力、σが*シグモイド関数*）

* 一つの層のたくさんある*パーセプトロン*のうち、一つがめっちゃ影響力強くなっちゃうことがある
  * それを避けるために、ランダムにdropoutする
  * [過学習](%E9%81%8E%E5%AD%A6%E7%BF%92.md)を避けるために

\#Udacity_Intro_to_Deep_Learning_with_PyTorch
\#ディープラーニング
